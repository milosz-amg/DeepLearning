{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059e63ac",
   "metadata": {},
   "source": [
    "# 03LAB - zadanie\n",
    "### Miłosz Rolewski"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c96b1",
   "metadata": {},
   "source": [
    "## Przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03be8c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "import csv\n",
    "\n",
    "SPECIALS = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "\n",
    "def load_data_tsv(path):\n",
    "    all_tokens = []\n",
    "    all_tags = []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        tsv_reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        for row in tsv_reader:\n",
    "            if len(row) != 2:\n",
    "                continue\n",
    "            tags_str, text_str = row\n",
    "            tags = tags_str.strip().split(\" \")\n",
    "            tokens = text_str.strip().split(\" \")\n",
    "            # usuwamy tagi </S>\n",
    "            filtered = [(t, tag) for t, tag in zip(tokens, tags) if t != \"</S>\"]\n",
    "            if not filtered:\n",
    "                continue\n",
    "            tokens, tags = zip(*filtered)\n",
    "            all_tokens.append(list(tokens))\n",
    "            all_tags.append(list(tags))\n",
    "\n",
    "    return all_tokens, all_tags\n",
    "\n",
    "def build_vocab(tokens_list):\n",
    "    counter = Counter()\n",
    "    for tokens in tokens_list:\n",
    "        counter.update(tokens)\n",
    "    v = vocab(counter, specials=SPECIALS)\n",
    "    v.set_default_index(v[\"<unk>\"])\n",
    "    return v\n",
    "\n",
    "def build_label_vocab(tag_list):\n",
    "    unique_tags = sorted(set(tag for tags in tag_list for tag in tags))\n",
    "    tag2id = {tag: i for i, tag in enumerate(unique_tags)}\n",
    "    id2tag = {i: tag for tag, i in tag2id.items()}\n",
    "    return tag2id, id2tag\n",
    "\n",
    "def vectorize(tokens_list, tags_list, token_vocab, tag_vocab):\n",
    "    token_ids = []\n",
    "    tag_ids = []\n",
    "    for tokens, tags in zip(tokens_list, tags_list):\n",
    "        token_vec = [token_vocab[\"<bos>\"]] + [token_vocab[t] for t in tokens] + [token_vocab[\"<eos>\"]]\n",
    "        tag_vec = [tag_vocab[\"O\"]] + [tag_vocab[t] for t in tags] + [tag_vocab[\"O\"]]  # \"O\" jako neutralny\n",
    "        token_ids.append(torch.tensor(token_vec, dtype=torch.long))\n",
    "        tag_ids.append(torch.tensor(tag_vec, dtype=torch.long))\n",
    "    return token_ids, tag_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b25ddaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.', 'Peter', 'Blackburn', 'BRUSSELS', '1996-08-22', 'The', 'European', 'Commission', 'said', 'on', 'Thursday', 'it', 'disagreed', 'with', 'German', 'advice', 'to', 'consumers', 'to', 'shun', 'British', 'lamb', 'until', 'scientists', 'determine', 'whether', 'mad', 'cow', 'disease', 'can', 'be', 'transmitted', 'to', 'sheep', '.', 'Germany', \"'s\", 'representative', 'to', 'the', 'European', 'Union', \"'s\", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.', '\"', 'We', 'do', \"n't\", 'support', 'any', 'such', 'recommendation', 'because', 'we', 'do', \"n't\", 'see', 'any', 'grounds', 'for', 'it', ',', '\"', 'the', 'Commission', \"'s\", 'chief', 'spokesman', 'Nikolaus', 'van', 'der', 'Pas', 'told', 'a', 'news', 'briefing', '.', 'He', 'said', 'further', 'scientific', 'study', 'was', 'required', 'and', 'if', 'it', 'was', 'found', 'that', 'action', 'was', 'needed', 'it', 'should', 'be', 'taken', 'by', 'the', 'European', 'Union', '.', 'He', 'said', 'a', 'proposal', 'last', 'month', 'by', 'EU', 'Farm', 'Commissioner', 'Franz', 'Fischler', 'to', 'ban', 'sheep', 'brains', ',', 'spleens', 'and', 'spinal', 'cords', 'from', 'the', 'human', 'and', 'animal', 'food', 'chains', 'was', 'a', 'highly', 'specific', 'and', 'precautionary', 'move', 'to', 'protect', 'human', 'health', '.', 'Fischler', 'proposed', 'EU-wide', 'measures', 'after', 'reports', 'from', 'Britain', 'and', 'France', 'that', 'under', 'laboratory', 'conditions', 'sheep', 'could', 'contract', 'Bovine', 'Spongiform', 'Encephalopathy', '(', 'BSE', ')', '--', 'mad', 'cow', 'disease', '.', 'But', 'Fischler', 'agreed', 'to', 'review', 'his', 'proposal', 'after', 'the', 'EU', \"'s\", 'standing', 'veterinary', 'committee', ',', 'mational', 'animal', 'health', 'officials', ',', 'questioned', 'if', 'such', 'action', 'was', 'justified', 'as', 'there', 'was', 'only', 'a', 'slight', 'risk', 'to', 'human', 'health', '.', 'Spanish', 'Farm', 'Minister', 'Loyola', 'de', 'Palacio', 'had', 'earlier', 'accused', 'Fischler', 'at', 'an', 'EU', 'farm', 'ministers', \"'\", 'meeting', 'of', 'causing', 'unjustified', 'alarm', 'through', '\"', 'dangerous', 'generalisation', '.', '\"', '.', 'Only', 'France', 'and', 'Britain', 'backed', 'Fischler', \"'s\", 'proposal', '.', 'The', 'EU', \"'s\", 'scientific', 'veterinary', 'and', 'multidisciplinary', 'committees', 'are', 'due', 'to', 're-examine', 'the', 'issue', 'early', 'next', 'month', 'and', 'make', 'recommendations', 'to', 'the', 'senior', 'veterinary', 'officials', '.', 'Sheep', 'have', 'long', 'been', 'known', 'to', 'contract', 'scrapie', ',', 'a', 'brain-wasting', 'disease', 'similar', 'to', 'BSE', 'which', 'is', 'believed', 'to', 'have', 'been', 'transferred', 'to', 'cattle', 'through', 'feed', 'containing', 'animal', 'waste', '.', 'British', 'farmers', 'denied', 'on', 'Thursday', 'there', 'was', 'any', 'danger', 'to', 'human', 'health', 'from', 'their', 'sheep', ',', 'but', 'expressed', 'concern', 'that', 'German', 'government', 'advice', 'to', 'consumers', 'to', 'avoid', 'British', 'lamb', 'might', 'influence', 'consumers', 'across', 'Europe', '.', '\"', 'What', 'we', 'have', 'to', 'be', 'extremely', 'careful', 'of', 'is', 'how', 'other', 'countries', 'are', 'going', 'to', 'take', 'Germany', \"'s\", 'lead', ',', '\"', 'Welsh', 'National', 'Farmers', \"'\", 'Union', '(', 'NFU', ')', 'chairman', 'John', 'Lloyd', 'Jones', 'said', 'on', 'BBC', 'radio', '.', 'Bonn', 'has', 'led', 'efforts', 'to', 'protect', 'public', 'health', 'after', 'consumer', 'confidence', 'collapsed', 'in', 'March', 'after', 'a', 'British', 'report', 'suggested', 'humans', 'could', 'contract', 'an', 'illness', 'similar', 'to', 'mad', 'cow', 'disease', 'by', 'eating', 'contaminated', 'beef', '.', 'Germany', 'imported', '47,600', 'sheep', 'from', 'Britain', 'last', 'year', ',', 'nearly', 'half', 'of', 'total', 'imports', '.', 'It', 'brought', 'in', '4,275', 'tonnes', 'of', 'British', 'mutton', ',', 'some', '10', 'percent', 'of', 'overall', 'imports', '.']\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-PER', 'I-PER', 'B-LOC', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "tensor([  2,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,  15,  16,\n",
      "         17,  18,  19,  20,  21,  22,  23,  24,  25,   6,  26,   8,  27,   8,\n",
      "         28,  10,  11,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,   8,\n",
      "         39,  12,  40,  41,  42,   8,  43,  18,  44,  41,  45,  46,  47,  48,\n",
      "         20,  21,  49,  27,  50,  51,  52,  53,  54,  55,  56,  57,  29,  43,\n",
      "         58,  26,  59,  60,  12,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  63,  64,  71,  66,  72,  73,  23,  74,  61,  43,  19,  41,  75,\n",
      "         76,  77,  78,  79,  80,  81,  82,  83,  84,  12,  85,  20,  86,  58,\n",
      "         87,  59,  88,  89,  90,  23,  59,  91,  92,  93,  59,  94,  23,  50,\n",
      "         37,  95,  96,  43,  18,  44,  12,  85,  20,  82,  97,  98,  99,  96,\n",
      "          4, 100, 101, 102, 103,   8, 104,  39, 105,  74, 106,  89, 107, 108,\n",
      "         53,  43, 109,  89, 110, 111, 112,  59,  82, 113, 114,  89, 115, 116,\n",
      "          8, 117, 109, 118,  12, 103, 119, 120, 121, 122, 123,  53,  57,  89,\n",
      "        124,  92, 125, 126, 127,  39, 128, 129, 130, 131, 132, 133, 134, 135,\n",
      "        136,  33,  34,  35,  12, 137, 103, 138,   8, 139, 140,  97, 122,  43,\n",
      "          4,  41, 141,  45,  46,  74, 142, 110, 118, 143,  74, 144,  90,  67,\n",
      "         93,  59, 145, 146, 147,  59, 148,  82, 149, 150,   8, 109, 118,  12,\n",
      "        151, 100, 152, 153, 154, 155, 156, 157, 158, 103, 159, 160,   4, 161,\n",
      "        162, 163, 164, 165, 166, 167, 168, 169,  61, 170, 171,  12,  61,  12,\n",
      "        172, 124,  89,  57, 173, 103,  41,  97,  12,  17,   4,  41,  58,  45,\n",
      "         89, 174, 175, 176, 177,   8, 178,  43, 179, 180, 181,  99,  89, 182,\n",
      "        183,   8,  43, 184,  45, 143,  12, 185, 186, 187, 188, 189,   8, 129,\n",
      "        190,  74,  82, 191,  35, 192,   8, 134, 193, 194, 195,   8, 186, 188,\n",
      "        196,   8, 197, 169, 198, 199, 110, 200,  12,  10, 201, 202,  21,  22,\n",
      "        147,  59,  66, 203,   8, 109, 118,  53, 204,  39,  74, 205, 206, 207,\n",
      "         92,   6, 208,  26,   8,  27,   8, 209,  10,  11, 210, 211,  27, 212,\n",
      "        213,  12,  61, 214,  70, 186,   8,  37, 215, 216, 165, 194, 217,  55,\n",
      "         54, 176, 218,   8, 219,  40,  41, 220,  74,  61, 221, 222, 223, 163,\n",
      "         44, 133, 224, 135, 225, 226, 227, 228,  20,  21, 229, 230,  12, 231,\n",
      "        232, 233, 234,   8, 117, 235, 118, 122, 236, 237, 238, 239, 240, 122,\n",
      "         82,  10, 241, 242, 243, 128, 129, 160, 244, 192,   8,  33,  34,  35,\n",
      "         96, 245, 246, 247,  12,  40, 248, 249,  39,  53,  57,  98, 250,  74,\n",
      "        251, 252, 165, 253, 254,  12, 255, 256, 239, 257, 258, 165,  10, 259,\n",
      "         74, 260, 261, 262, 165, 263, 254,  12,   3])\n",
      "tensor([8, 2, 8, 1, 8, 8, 8, 1, 8, 8, 3, 7, 0, 8, 8, 2, 6, 8, 8, 8, 8, 8, 8, 1,\n",
      "        8, 8, 8, 8, 8, 1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 8, 8, 8,\n",
      "        8, 2, 6, 8, 8, 8, 3, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 2,\n",
      "        8, 8, 8, 3, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 2, 6, 8, 8, 8, 8, 8, 8, 8, 8, 2, 8, 8, 3,\n",
      "        7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 3, 8, 1, 8, 8, 8, 8, 0, 8, 0, 8, 8, 8, 8, 8, 8, 8, 1, 5,\n",
      "        5, 8, 1, 8, 8, 8, 8, 8, 8, 8, 3, 8, 8, 8, 8, 8, 8, 8, 2, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 8,\n",
      "        8, 3, 7, 7, 8, 8, 8, 3, 8, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 0, 8, 0, 8, 3, 8, 8, 8, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 8, 8, 8, 8, 8, 8, 1, 8,\n",
      "        8, 8, 8, 8, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0,\n",
      "        8, 8, 8, 8, 2, 6, 6, 6, 6, 8, 2, 8, 8, 3, 7, 7, 8, 8, 2, 6, 8, 0, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 0, 8, 8, 8, 8, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "tokens, tags = load_data_tsv(\"en-ner-conll-2003/train/train.tsv\")\n",
    "token_vocab = build_vocab(tokens)\n",
    "tag2id, id2tag = build_label_vocab(tags)\n",
    "token_ids, tag_ids = vectorize(tokens, tags, token_vocab, tag2id)\n",
    "\n",
    "print(tokens[0])\n",
    "print(tags[0])\n",
    "print(token_ids[0])\n",
    "print(tag_ids[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5641d79",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff6e81e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels, embedding_dim=100, hidden_dim=256):\n",
    "        super(LSTM_NER, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                   # (batch, seq_len, embed_dim)\n",
    "        x, _ = self.lstm(x)                     # (batch, seq_len, hidden_dim)\n",
    "        logits = self.classifier(x)             # (batch, seq_len, num_labels)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12006ea",
   "metadata": {},
   "source": [
    "## Przygotowanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf9b641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LSTM_NER(vocab_size=len(token_vocab), num_labels=len(tag2id)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa4e5f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [00:16<00:00, 57.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 584.1406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [00:16<00:00, 55.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 308.8680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [00:17<00:00, 55.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 182.4637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [00:16<00:00, 56.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss: 108.3680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [00:17<00:00, 55.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 loss: 59.6965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for tokens_tensor, tags_tensor in tqdm(zip(token_ids, tag_ids), total=len(tag_ids)):\n",
    "        tokens_tensor = tokens_tensor.unsqueeze(0).to(device)  # batch = 1\n",
    "        tags_tensor = tags_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(tokens_tensor)  # (1, seq_len, num_labels)\n",
    "        loss = criterion(output.squeeze(0), tags_tensor.squeeze(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1de63e",
   "metadata": {},
   "source": [
    "## Ewaluacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63b081aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "def evaluate_model(model, token_ids, tag_ids, tag_vocab, id2tag):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for tokens_tensor, tags_tensor in zip(token_ids, tag_ids):\n",
    "            tokens_tensor = tokens_tensor.unsqueeze(0).to(device)\n",
    "            tags_tensor = tags_tensor.to(device)\n",
    "\n",
    "            outputs = model(tokens_tensor)  # (1, seq_len, num_labels)\n",
    "            predictions = torch.argmax(outputs.squeeze(0), dim=1).cpu().tolist()\n",
    "            true_tags = tags_tensor.cpu().tolist()\n",
    "\n",
    "            # Pomijamy <bos> i <eos>\n",
    "            pred_tags = [id2tag[i] for i in predictions[1:-1]]\n",
    "            true_tags = [id2tag[i] for i in true_tags[1:-1]]\n",
    "\n",
    "            y_pred.append(pred_tags)\n",
    "            y_true.append(true_tags)\n",
    "\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"F1-score:\", f1_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4959614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.92      0.93      0.93      7139\n",
      "        MISC       0.82      0.88      0.85      3436\n",
      "         ORG       0.82      0.87      0.85      6317\n",
      "         PER       0.92      0.88      0.90      6600\n",
      "\n",
      "   micro avg       0.88      0.89      0.89     23492\n",
      "   macro avg       0.87      0.89      0.88     23492\n",
      "weighted avg       0.88      0.89      0.89     23492\n",
      "\n",
      "F1-score: 0.8861438405338062\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, token_ids, tag_ids, tag2id, id2tag)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
